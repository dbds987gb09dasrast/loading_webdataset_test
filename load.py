import os
import sys
from glob import glob
from datetime import datetime

import numpy as np
import cv2
import webdataset as wds
import braceexpand

from torch.utils.data import DataLoader
from torchvision.transforms import Compose, RandomResizedCrop, Normalize, ToTensor


normalize = Normalize(
    mean=[0.485, 0.456, 0.406],
    std=[0.229, 0.224, 0.225])

preproc = Compose([
    RandomResizedCrop(384),
    ToTensor(),
    normalize])


def identity(x):
    return x


def single_node():
    """
    Assumed ${data_root} as the directory generated by the following command:
    img2dataset --url_list laion400m-meta --input_format "parquet" --url_col "URL" --caption_col "TEXT" --output_format webdataset --output_folder ${data_root}
    """

    data_root= "/path/to/laion-400m"
    data_index= [int(os.path.splitext(os.path.basename(fl))[0]) for fl in glob(os.path.join(data_root, "*.tar"))]
    data_index.sort()
    print(len(data_index))
    print(data_index[:5])

    batch_path= os.path.join(data_root, "{:05d}.tar".format(data_index[192]))

    # start1= datetime.now()
    # ds= wds.WebDataset(batch_path)
    # ds= ds.decode("rgb").to_tuple("webp", "json")
    # start2= datetime.now()
    # img, js= next(iter(ds))
    # print(np.max(img))
    # cv2.imwrite("prev.png", (img*255).astype(np.uint8))
    # print(img.shape)
    # print(js)

    start1= datetime.now()
    ds = (
        wds.WebDataset(batch_path)
        .decode("pil")
        .to_tuple("webp", "json")
        .map_tuple(preproc, identity))
    
    start2= datetime.now()
    batch_size = 64
    dl = DataLoader(ds.batched(batch_size), num_workers=2, batch_size=None, pin_memory=True)
    start3= datetime.now()
    for img, trg in dl:
        # print(img.shape)
        pass
    allend= datetime.now()

    print("Input: ", img.shape)
    print("Output", len(trg))
    print(trg[0])

    print(start2-start1)
    print(start3-start2)
    print(allend-start3)
    # SATA
    #bs= 64: worker=2~0:00:18.701800,  worker=4~0:00:19.007674, worker=8~0:00:18.954796
    #bs=128: worker=2~0:00:18.952625,  worker=4~0:00:18.693086, worker=8~0:00:19.020247

    # SSD
    #bs= 64: worker=2~0:00:18.991907,  worker=4~0:00:18.950253, worker=8~0:00:18.980862
    #bs=128: worker=2~0:00:18.697496,  worker=4~0:00:18.702886, worker=8~0:00:18.980204



def multi_node():
    # data_root= "/path/to/laion-400m"
    data_root= "temp"
    dslist= list(braceexpand.braceexpand(os.path.join(data_root, "{00000..00004}.tar")))
    ds = (
        wds.WebDataset(dslist)
        .decode("pil")
        .to_tuple("webp", "json")
        .map_tuple(preproc, identity))
    
    start2= datetime.now()
    batch_size = 64
    dl = DataLoader(ds.batched(batch_size), num_workers=2, batch_size=None, pin_memory=True)
    # dl = dl.ddp_equalize(1000 // batch_size)
    start3= datetime.now()
    cnt= 0
    for img, trg in dl:
        # print(img.shape)
        cnt+=1
    allend= datetime.now()

    print("Loaded: ", cnt)
    print("Input: ", img.shape)
    print("Output", len(trg))
    print(trg[0])

    print(allend-start3)
    # SATA
    #bs= 64: worker=2~0:00:18.701800,  worker=4~0:00:19.007674, worker=8~0:00:18.954796
    #bs=128: worker=2~0:00:18.952625,  worker=4~0:00:18.693086, worker=8~0:00:19.020247

    # SSD
    #bs= 64: worker=2~0:00:18.991907,  worker=4~0:00:18.950253, worker=8~0:00:18.980862
    #bs=128: worker=2~0:00:18.697496,  worker=4~0:00:18.702886, worker=8~0:00:18.980204

    # SATA
    #bs= 64: worker=2~0:00:54.244096,  worker=4~0:00:36.474338, worker=8~0:00:19.662711
    #bs=128: worker=2~0:00:52.768528,  worker=4~0:00:35.903016, worker=8~0:00:19.479905

    # SSD
    #bs= 64: worker=2~0:00:53.362472,  worker=4~0:00:36.205736, worker=8~0:00:19.499948
    #bs=128: worker=2~0:00:52.200180,  worker=4~0:00:35.864480, worker=8~0:00:19.719000



if __name__=="__main__":
    multi_node()
